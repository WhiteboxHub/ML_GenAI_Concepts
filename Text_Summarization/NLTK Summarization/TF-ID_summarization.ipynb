{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scraping(url):\n",
    "    # Define browser-like headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Sending request to get the data from the webpage\n",
    "        scraped_data = requests.get(url, headers=headers)\n",
    "        scraped_data.raise_for_status()  # Check if request was successful (status code 200)\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(scraped_data.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs and extract text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_context = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_context += p.get_text(strip=True) + \" \"  # Strip whitespace and concatenate paragraphs\n",
    "\n",
    "        return article_context.strip()  # Return the article content, ensuring no leading/trailing whitespace\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised by requests\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text document\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a sub-field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
    "The ultimate goal of NLP is to enable computers to understand, interpret, and respond to human language in a way that is both valuable and meaningful.\n",
    "Applications of NLP are used in a wide range of industries including customer service, healthcare, finance, and more. NLP helps businesses automate repetitive tasks, analyze vast amounts of text data, and gain insights from customer interactions.\n",
    "\"\"\"\n",
    "url = \"https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)\"\n",
    "text = scraping(url)\n",
    "# Tokenize the document into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the average TF-IDF score for each sentence\n",
    "sentence_scores = tfidf_matrix.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to get the top scored sentences indices\n",
    "def top_n_indices(arr, num_sentences = 10):\n",
    "    # Flatten the array to 1D, get the indices of the top n values\n",
    "    indices = np.argsort(arr.flatten())[-num_sentences:]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort sentences by their scores and select the top N sentences for the summary\n",
    "num_sentences = 2  # Number of sentences for the summary\n",
    "top_sentence_indices = top_n_indices(np.array(sentence_scores),num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteFile(final_summary):\n",
    "    with open('../summary.txt',mode='a', encoding='utf-8') as file:\n",
    "        file.write(f\"\\n \\n \\n The TF-ID summarized text using NLTK is as follows : \\n {final_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "[3] For some architectures, such asconvolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-levelfeatures, while later layers often discern high-level features that can be more related to the task that the model is trained on. Fine-tuning is common innatural language processing(NLP), especially in the domain oflanguage modeling.Large language modelslikeOpenAI's series ofGPT foundation modelscan be fine-tuned on data for specific downstream NLP tasks (tasks that use a pre-trained model) to improve performance over the unmodified pre-trained model.\n"
     ]
    }
   ],
   "source": [
    "# Sort the selected sentences by their order of appearance\n",
    "sorted_top_sentence_indices = sorted(top_sentence_indices)\n",
    "# Create the summary\n",
    "summary = \" \".join([sentences[i] for i in sorted_top_sentence_indices])\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "WriteFile(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
