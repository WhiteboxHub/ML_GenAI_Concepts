{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk sklearn pandas numpy sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Loading and Previewing SMS Spam Dataset\n",
    "\n",
    "1. **Loading the Dataset**:\n",
    "   - The code uses `pandas` to load a text file into a DataFrame. The function `pd.read_table()` reads the file `SMSSpamCollection` located in the `./classification_data/` directory.\n",
    "   - The file is expected to be a tab-separated values (TSV) file, which means each column is separated by a tab character.\n",
    "\n",
    "2. **No Header in the Dataset**:\n",
    "   - The argument `header=None` specifies that the dataset does not have a predefined header (column names). Thus, pandas will automatically assign default integer column names (starting from 0).\n",
    "\n",
    "3. **Encoding**:\n",
    "   - The `encoding='utf-8'` parameter ensures that the text is read correctly in case of any special characters, preventing errors during the file loading process.\n",
    "\n",
    "4. **Previewing the Data**:\n",
    "   - `sms.head()` displays the first five rows of the DataFrame, providing a quick view of the loaded data. This is useful for confirming the structure and contents of the dataset.\n",
    "\n",
    "5. **Usage**:\n",
    "   - The dataset likely contains SMS text messages labeled as either spam or not spam, which is common in classification tasks. The loaded DataFrame can now be used for further data analysis or machine learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = pd.read_table('./classification_data/SMSSpamCollection', header=None, encoding='utf-8')\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Accessing the First Column**:\n",
    "   - `sms[0]` accesses the first column of the DataFrame `sms`. Since the dataset does not have predefined column names, pandas assigned integer-based labels (starting from 0). \n",
    "   - This first column likely contains the labels (e.g., \"spam\" or \"ham\" for non-spam messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Label Encoding for SMS Spam Classification\n",
    "\n",
    "1. **Importing LabelEncoder**:\n",
    "   - The `LabelEncoder` from `sklearn.preprocessing` is imported. This is used to transform categorical labels into numeric values (i.e., encoding string labels like \"spam\" or \"ham\" into integers).\n",
    "\n",
    "2. **Fitting and Transforming Labels**:\n",
    "   - The `enc.fit_transform(sms[0])` applies the label encoder to the first column of the `sms` dataset, which contains the target labels (e.g., \"spam\", \"ham\"). This converts them into integer values (e.g., \"spam\" might become 1, and \"ham\" might become 0).\n",
    "   \n",
    "3. **Printing Encoded and Original Labels**:\n",
    "   - The first 10 encoded labels (`label[:10]`) and the original 10 labels (`sms[0][:10]`) are printed. This allows you to compare how the labels have been transformed from text to numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n",
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "5    spam\n",
      "6     ham\n",
      "7     ham\n",
      "8    spam\n",
      "9    spam\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label = enc.fit_transform(sms[0])\n",
    "print(label[:10])\n",
    "print(sms[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "5    FreeMsg Hey there darling it's been 3 week's n...\n",
       "6    Even my brother is not like to speak with me. ...\n",
       "7    As per your request 'Melle Melle (Oru Minnamin...\n",
       "8    WINNER!! As a valued network customer you have...\n",
       "9    Had your mobile 11 months or more? U R entitle...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sms[1]\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace email addresses with 'email'\n",
    "processed = text.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddress', regex=True)\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', 'webaddress', regex=True)\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb', regex=True)\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumbr', regex=True)\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr', regex=True)\n",
    "\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ', regex=True)\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point crazy available only in ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry in numbr a wkly comp to win fa cup ...\n",
       "3             u dun say so early hor u c already then say\n",
       "4       nah i don t think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    this is the numbrnd time we have tried numbr c...\n",
       "5568                  will ü b going to esplanade fr home\n",
       "5569    pity was in mood for that so any other suggest...\n",
       "5570    the guy did some bitching but i acted like i d...\n",
       "5571                            rofl its true to its name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convering the strings to lower case\n",
    "processed = processed.str.lower()\n",
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Text Preprocessing with Stopwords Removal and Stemming\n",
    "\n",
    "1. **Loading Stopwords**:\n",
    "   - The `stopwords` from `nltk.corpus` are loaded and stored in the `stop_words` variable. These are common words in English (e.g., \"is\", \"the\", \"in\") that are typically removed in text preprocessing to focus on more meaningful words.\n",
    "\n",
    "2. **Stopwords Removal**:\n",
    "   - The `processed` dataset is updated by applying a lambda function that removes stopwords. The function splits each text string into terms (words), filters out any term found in the `stop_words` set, and rejoins the remaining terms into a cleaned sentence.\n",
    "\n",
    "3. **Porter Stemming**:\n",
    "   - The `nltk.PorterStemmer()` is instantiated as `ps`. Stemming reduces words to their root form (e.g., \"running\" becomes \"run\").\n",
    "\n",
    "4. **Applying Stemming**:\n",
    "   - Another lambda function is applied to `processed`, where each word in the text is stemmed using `ps.stem()`, and then the terms are rejoined to form the processed sentence.\n",
    "\n",
    "5. **Output**:\n",
    "   - The fully processed `processed` data is displayed, showing the text after stopwords have been removed and terms have been stemmed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazi avail bugi n great world...\n",
       "1                                   ok lar joke wif u oni\n",
       "2       free entri numbr wkli comp win fa cup final tk...\n",
       "3                     u dun say earli hor u c alreadi say\n",
       "4                    nah think goe usf live around though\n",
       "                              ...                        \n",
       "5567    numbrnd time tri numbr contact u u moneysymbnu...\n",
       "5568                              ü b go esplanad fr home\n",
       "5569                                    piti mood suggest\n",
       "5570    guy bitch act like interest buy someth els nex...\n",
       "5571                                       rofl true name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6579\n",
      "Most common words: [('numbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 451), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('moneysymbnumbr', 303), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Print the result\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Tokenization and Word Frequency Distribution\n",
    "\n",
    "1. **Tokenizing Words**:\n",
    "   - The `word_tokenize` function from `nltk` is used to split each message in the `processed` dataset into individual words (tokens). The result is a list of words for each message.\n",
    "\n",
    "2. **Building Word List**:\n",
    "   - For each message, the words are added to a cumulative list called `all_words`, which stores every word found in the processed text data.\n",
    "\n",
    "3. **Frequency Distribution**:\n",
    "   - The `nltk.FreqDist` function is used to create a frequency distribution of the words stored in `all_words`. This helps in counting how often each word appears across the dataset.\n",
    "\n",
    "4. **Result Output**:\n",
    "   - The total number of unique words is printed using `len(all_words)`.\n",
    "   - The `most_common(15)` method is called on `all_words` to print the 15 most frequently occurring words along with their counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [x[0] for x in all_words.most_common(1500)]\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Feature Extraction Using Word Frequencies\n",
    "\n",
    "1. **Creating Word Features**:\n",
    "   - `word_features` is a list that stores the top 1,500 most common words from the frequency distribution `all_words`. These words will act as features for the model.\n",
    "\n",
    "2. **`find_features` Function**:\n",
    "   - The function `find_features` takes a message as input and tokenizes it into individual words using `word_tokenize`.\n",
    "   - A dictionary `features` is created, where the keys are the words from `word_features`, and the values are `True` or `False` depending on whether the word appears in the message or not.\n",
    "\n",
    "3. **Output**:\n",
    "   - The function returns a dictionary where each key (word from `word_features`) maps to a boolean (`True` or `False`), indicating whether that word is present in the message. This dictionary can be used as feature input for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "got\n",
      "n\n",
      "great\n",
      "wat\n",
      "e\n",
      "world\n",
      "point\n",
      "avail\n",
      "crazi\n",
      "bugi\n",
      "la\n",
      "cine\n"
     ]
    }
   ],
   "source": [
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Displaying Feature Words Present in a Message\n",
    "\n",
    "1. **Finding Features in a Processed Message**:\n",
    "   - The function `find_features(processed[0])` is called on the first message in the `processed` dataset. This identifies which of the top 1,500 most common words (`word_features`) are present in the message.\n",
    "\n",
    "2. **Iterating Over the Features**:\n",
    "   - A `for` loop is used to iterate through the dictionary `features`. Each key represents a word from `word_features`, and each value is a boolean (`True` or `False`) indicating the presence of the word in the message.\n",
    "\n",
    "3. **Printing Present Words**:\n",
    "   - The condition `if value == True:` checks if a word is present in the message. If the word is found, it is printed.\n",
    "   - This loop outputs all words from the top 1,500 that are present in the message.\n",
    "\n",
    "4. **Result**:\n",
    "   - The printed words will be the words from the top 1,500 most frequent words that appear in the given message (`processed[0]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(features.items())[:10]\n",
    "messages = list(zip(processed, label))\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# Call find_features function for each SMS message\n",
    "feature_set = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Preparing Feature Set for SMS Messages\n",
    "\n",
    "1. **Listing Features**:\n",
    "   - `list(features.items())[:10]` retrieves the first 10 items from the `features` dictionary, which contains the words and their presence in the processed message. This helps to visualize which words were considered as features.\n",
    "\n",
    "2. **Combining Processed Messages and Labels**:\n",
    "   - The `messages` variable is created by zipping together `processed` (the preprocessed SMS messages) and `label` (the corresponding labels indicating if a message is spam or not). This results in a list of tuples, where each tuple contains a message and its label.\n",
    "\n",
    "3. **Shuffling Messages**:\n",
    "   - `np.random.seed(1)` sets the random seed for reproducibility, ensuring that the shuffle operation produces the same order each time it is run.\n",
    "   - `np.random.shuffle(messages)` randomly shuffles the `messages` list to ensure that the training data is mixed, which is important for training machine learning models to avoid any order biases.\n",
    "\n",
    "4. **Creating the Feature Set**:\n",
    "   - A list comprehension is used to create `feature_set`, which contains tuples of features extracted from each message and their corresponding label. \n",
    "   - For each message (`text`), the `find_features(text)` function is called to generate a dictionary of features indicating the presence of the top words, and this is paired with the message's label.\n",
    "\n",
    "5. **Result**:\n",
    "   - The `feature_set` now contains a collection of features and labels, ready for use in training a machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, test = train_test_split(feature_set, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Training and Testing Sets\n",
    "\n",
    "1. **Importing Required Function**:\n",
    "   - The `train_test_split` function from the `sklearn.model_selection` module is imported. This function is commonly used to split datasets into training and testing subsets for machine learning.\n",
    "\n",
    "2. **Splitting the Dataset**:\n",
    "   - The line `training, test = train_test_split(feature_set, test_size=0.25, random_state=1)` performs the dataset split.\n",
    "     - **`feature_set`**: This is the dataset containing tuples of features and their corresponding labels, which was prepared earlier.\n",
    "     - **`test_size=0.25`**: This parameter specifies that 25% of the data should be reserved for testing, while the remaining 75% will be used for training the model.\n",
    "     - **`random_state=1`**: Setting the random seed ensures that the split is reproducible, meaning that every time this code is run with the same seed, the split will be identical.\n",
    "\n",
    "3. **Result**:\n",
    "   - The result of this operation is two separate datasets: \n",
    "     - **`training`**: Contains 75% of the `feature_set` data for training the model.\n",
    "     - **`test`**: Contains 25% of the `feature_set` data for evaluating the model's performance after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors model Accuracy: 0.9454414931801867\n",
      "Decision Tree model Accuracy: 0.95908111988514\n",
      "Random Forest model Accuracy: 0.9813352476669059\n",
      "Logistic Regression model Accuracy: 0.9834888729361091\n",
      "SGD Classifier model Accuracy: 0.9806173725771715\n",
      "Naive Bayes model Accuracy: 0.9856424982053122\n",
      "Support Vector Classifier model Accuracy: 0.9820531227566404\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier',\n",
    "         'Naive Bayes', 'Support Vector Classifier']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, test)\n",
    "    print(\"{} model Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Evaluating Multiple Classifiers\n",
    "\n",
    "1. **Importing Necessary Libraries**:\n",
    "   - The code imports various classifiers from `sklearn`, including:\n",
    "     - `KNeighborsClassifier`\n",
    "     - `DecisionTreeClassifier`\n",
    "     - `RandomForestClassifier`\n",
    "     - `LogisticRegression`\n",
    "     - `SGDClassifier`\n",
    "     - `MultinomialNB`\n",
    "     - `SVC` (Support Vector Classifier)\n",
    "   - It also imports metrics for model evaluation, such as `classification_report`, `accuracy_score`, and `confusion_matrix`.\n",
    "\n",
    "2. **Setting Up Classifiers**:\n",
    "   - A list named `names` contains the names of each classifier for easy reference.\n",
    "   - Another list named `classifiers` holds instances of the corresponding classifier classes.\n",
    "\n",
    "3. **Combining Names and Classifiers**:\n",
    "   - The `zip` function is used to pair each classifier name with its respective classifier instance, resulting in an iterable of tuples called `models`.\n",
    "\n",
    "4. **Training and Evaluating Classifiers**:\n",
    "   - A loop iterates through each `name` and `model` in `models`:\n",
    "     - For each classifier, it initializes an `nltk_model` using `SklearnClassifier`, which wraps the scikit-learn classifiers to integrate with NLTK.\n",
    "     - The model is trained using the `train` method with the `training` dataset.\n",
    "     - The accuracy of the model is calculated using `nltk.classify.accuracy`, which takes the trained model and the `test` dataset as inputs.\n",
    "     - The accuracy for each model is printed in a formatted string indicating which model was evaluated.\n",
    "\n",
    "5. **Output**:\n",
    "   - The output displays the accuracy for each classifier, allowing for a comparison of their performance on the given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier model Accuracy: 0.9806173725771715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Since VotingClassifier can accept list type of models\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators=models, voting='hard', n_jobs=-1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, test)\n",
    "print(\"Voting Classifier model Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Implementing a Voting Classifier\n",
    "\n",
    "1. **Importing VotingClassifier**:\n",
    "   - The code imports the `VotingClassifier` from the `sklearn.ensemble` module, which is used to combine multiple models into a single ensemble model for improved prediction accuracy.\n",
    "\n",
    "2. **Preparing the Models**:\n",
    "   - The previously created lists `names` and `classifiers` are combined using the `zip` function to create a list of tuples called `models`, where each tuple contains a classifier name and its corresponding instance.\n",
    "\n",
    "3. **Creating the Voting Classifier**:\n",
    "   - An instance of `VotingClassifier` is created, where:\n",
    "     - The `estimators` parameter is set to the `models` list, indicating the individual classifiers that will be part of the ensemble.\n",
    "     - The `voting` parameter is set to `'hard'`, meaning that the classifier will predict the class label based on the majority vote among the individual classifiers.\n",
    "     - The `n_jobs` parameter is set to `-1`, allowing the use of all available CPU cores for parallel processing during training.\n",
    "\n",
    "4. **Training the Ensemble Model**:\n",
    "   - The ensemble model is wrapped with `SklearnClassifier`, allowing it to integrate with NLTK's classification framework.\n",
    "   - The `train` method is called with the `training` dataset to fit the ensemble model.\n",
    "\n",
    "5. **Evaluating the Model**:\n",
    "   - The accuracy of the ensemble model is computed using `nltk.classify.accuracy`, which takes the trained model and the `test` dataset as inputs.\n",
    "   - Finally, the accuracy of the Voting Classifier is printed, providing insight into its performance compared to the individual classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1199\n",
      "           1       0.99      0.87      0.93       194\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_features, labels = zip(*test)\n",
    "prediction = nltk_ensemble.classify_many(text_features)\n",
    "print(classification_report(labels, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>25</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1197    2\n",
       "       spam        25  169"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame( confusion_matrix(labels, prediction),\n",
    "             index=[['actual', 'actual'], ['ham', 'spam']],\n",
    "             columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Evaluating the Voting Classifier Model\n",
    "\n",
    "1. **Extracting Test Features and Labels**:\n",
    "   - The code uses `zip(*test)` to separate the `test` dataset into two tuples: `text_features` and `labels`. \n",
    "   - `text_features` contains the feature sets of the SMS messages, while `labels` contains the corresponding true labels (ham or spam).\n",
    "\n",
    "2. **Making Predictions**:\n",
    "   - The `classify_many` method of the `nltk_ensemble` model is called with `text_features` to generate predictions for the test dataset. \n",
    "   - The predicted labels are stored in the `prediction` variable.\n",
    "\n",
    "3. **Generating a Classification Report**:\n",
    "   - The `classification_report` function from `sklearn.metrics` is used to evaluate the performance of the model. \n",
    "   - It provides a detailed report including precision, recall, f1-score, and support for each class (ham and spam) based on the true labels (`labels`) and predicted labels (`prediction`).\n",
    "\n",
    "4. **Creating a Confusion Matrix**:\n",
    "   - The `confusion_matrix` function generates a confusion matrix to visualize the model's performance in terms of true positives, false positives, true negatives, and false negatives.\n",
    "   - A Pandas DataFrame is created to display the confusion matrix with appropriate row and column labels for better readability. The matrix includes 'actual' and 'predicted' labels, allowing for easy interpretation of the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
