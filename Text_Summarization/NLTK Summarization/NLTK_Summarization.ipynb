{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: click in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#installing the requried libraries\n",
    "\n",
    "%pip install nltk beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Overview**: \n",
    "   This code snippet provides a function `get_word_weights(context)` that calculates and returns the normalized frequency of each unique word in a given text input. It filters out common stop words and punctuation to focus on meaningful terms.\n",
    "\n",
    "2. **Dependencies**: \n",
    "   The code requires the Natural Language Toolkit (NLTK) library. The following NLTK resources are downloaded to support tokenization and stop word filtering:\n",
    "   - `punkt`: For tokenizing text into words.\n",
    "   - `wordnet` and `omw-1.4`: For potential lemmatization and synonym functionalities (not used directly in this function).\n",
    "   - `stopwords`: To remove common English words that do not contribute to the text's meaning.\n",
    "\n",
    "3. **Function Usage**: \n",
    "   - To use the function, import the necessary libraries and run the NLTK download commands before invoking `get_word_weights()`.\n",
    "   - Call the function with a string argument containing the text you want to analyze, e.g., `weights = get_word_weights(\"Your text here\")`.\n",
    "\n",
    "4. **Output**: \n",
    "   The function returns a dictionary where each key is a unique word from the input text, and the corresponding value is its normalized frequency (between 0 and 1), representing its relative importance in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajukh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajukh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "def get_word_weights(context):\n",
    "    word_tokens = word_tokenize(context)            #.,$ []\n",
    "    punctuations = punctuation + '\\n'\n",
    "    # \",.()[]!@#$%^&*\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    # is , iam ,this ,they etc..\n",
    "\n",
    "    word_freqeuencies = {}\n",
    "    for word in word_tokens:\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuations:\n",
    "                if word not in word_freqeuencies.keys():\n",
    "                    word_freqeuencies[word] = 1\n",
    "                else:\n",
    "                    word_freqeuencies[word] += 1\n",
    "    max_frequency = max(word_freqeuencies.values())\n",
    "    for word in word_freqeuencies.keys():\n",
    "        word_freqeuencies[word] = word_freqeuencies[word]/max_frequency\n",
    "        \n",
    "    return word_freqeuencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Weight Calculation Function\n",
    "\n",
    "1. **Purpose**: The `get_sentence_weights` function calculates the weight of each sentence in a given text based on the frequency of words, helping to identify the most significant sentences.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - `context`: A string containing the text for which sentence weights are to be computed.\n",
    "   - `word_frequency`: A dictionary where each key represents a word and its value indicates the word's frequency within the text.\n",
    "\n",
    "3. **Process**: The function tokenizes the input text into sentences and iterates through each sentence. It calculates the weight of each sentence by summing the frequencies of the words it contains, excluding stop words.\n",
    "\n",
    "4. **Output**: The function returns a dictionary where each key is a sentence from the input text, and the corresponding value is the cumulative weight based on the word frequencies.\n",
    "\n",
    "5. **Usage**: This function can be used in text summarization tasks to determine which sentences carry the most weight and should be included in a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_weights(context,word_frequency):\n",
    "    sentence = sent_tokenize(context)\n",
    "    sentence_weight = dict()\n",
    "    for line in sentence:\n",
    "        sentence_wordcount = len(word_tokenize(line))\n",
    "        sentence_wordcount_without_stop_word = 0\n",
    "        for word in word_tokenize(line.lower()):\n",
    "            if word in word_frequency.keys():\n",
    "                sentence_wordcount_without_stop_word += 1\n",
    "                if line in sentence_weight:\n",
    "                    sentence_weight[line] += word_frequency[word]\n",
    "                else:\n",
    "                    sentence_weight[line] = word_frequency[word]\n",
    "                    \n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Summary Generation Function\n",
    "\n",
    "1. **Purpose**: The `get_sentence_summary` function generates a summary of text based on the weights of its sentences, allowing for concise representation of the original content.\n",
    "\n",
    "2. **Parameters**:\n",
    "   - `sentence_weight`: A dictionary where each key is a sentence and the value represents its calculated weight.\n",
    "   - `summary_len`: A float (default value 0.5) indicating the proportion of sentences to include in the summary relative to the total number of sentences.\n",
    "\n",
    "3. **Process**: The function calculates the number of sentences to include in the summary by multiplying the total number of sentences by `summary_len`. It then selects the top-weighted sentences using the `nlargest` function.\n",
    "\n",
    "4. **Output**: The function returns a single string that contains the selected sentences, joined together to form a coherent summary.\n",
    "\n",
    "5. **Usage**: This function is useful in text summarization tasks, enabling the extraction of key information from larger texts by selecting the most important sentences based on their weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_summary(sentence_weight,summary_len = 0.5):\n",
    "    select_len = int(len(sentence_weight) * summary_len)\n",
    "    summary = nlargest(select_len,sentence_weight,key = sentence_weight.get)\n",
    "    #a array of senteces with highest weight \n",
    "    final_summary = \" \".join(summary)\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Function\n",
    "\n",
    "1. **Purpose**: The `scraping` function retrieves text content from a specified webpage URL by making an HTTP request and parsing the HTML content.\n",
    "\n",
    "2. **Headers**: The function utilizes browser-like headers to mimic a real browser request, which helps prevent potential blocking by the server due to automated scraping.\n",
    "\n",
    "3. **Data Retrieval**: The function sends a GET request to the specified URL. If the request is successful (status code 200), it proceeds to parse the HTML content.\n",
    "\n",
    "4. **Content Extraction**: Using BeautifulSoup, the function searches for all paragraph (`<p>`) tags in the HTML and concatenates their text content into a single string, stripping any leading or trailing whitespace.\n",
    "\n",
    "5. **Error Handling**: The function includes error handling to catch and display exceptions that may occur during the request, ensuring robustness in case of network issues or invalid URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scraping(url):\n",
    "    # Define browser-like headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Sending request to get the data from the webpage\n",
    "        scraped_data = requests.get(url, headers=headers)\n",
    "        scraped_data.raise_for_status()  # Check if request was successful (status code 200)\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(scraped_data.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs and extract text\n",
    "        # p stands for paragraph element in html\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_context = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_context += p.get_text(strip=True) + \" \"  # Strip whitespace and concatenate paragraphs\n",
    "\n",
    "        return article_context.strip()  # Return the article content, ensuring no leading/trailing whitespace\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised by requests\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Loading Function\n",
    "\n",
    "1. **Purpose**: The `load_files` function is designed to read and load text files from a specified directory, storing their contents in a list.\n",
    "\n",
    "2. **Directory Input**: The function accepts a single argument, `directory`, which is the path to the directory containing the text files to be loaded.\n",
    "\n",
    "3. **File Reading**: It iterates through all files in the given directory. For each file, it opens the file in read mode and appends its contents to a list.\n",
    "\n",
    "4. **Data Storage**: The contents of each file are collected in the `files_data` list, which holds the text from all files in the specified directory.\n",
    "\n",
    "5. **Return Value**: The function returns the `files_data` list, providing the caller with the text content of all files in the specified directory for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "def load_files(directory):\n",
    "    files_data = []\n",
    "\n",
    "    for fname in os.listdir(directory):\n",
    "        with open(f\"{directory}/{fname}\",'r') as f:\n",
    "            files_data.append(f.read())\n",
    "    \n",
    "    return files_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Writing Function\n",
    "\n",
    "1. **Purpose**: The `WriteFile` function is designed to write the summarized text and the original context into a file named `summary.txt`.\n",
    "\n",
    "2. **Parameters**: The function accepts two parameters: \n",
    "   - `context`: The original text that has been summarized.\n",
    "   - `final_summary`: The summarized version of the original text.\n",
    "\n",
    "3. **File Handling**: The function opens (or creates) the `summary.txt` file in append mode (`'a'`), ensuring that new summaries are added without overwriting existing content.\n",
    "\n",
    "4. **Content Structure**: The function writes a structured format to the file, including:\n",
    "   - A header indicating the summarized text.\n",
    "   - The summarized text itself.\n",
    "   - Separators to visually distinguish between different summaries.\n",
    "   - A header for the original text followed by the original context.\n",
    "\n",
    "5. **Encoding**: The file is written using UTF-8 encoding to support a wide range of characters and ensure proper storage of the text content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesdata = load_files('./sample_mail_data')\n",
    "\n",
    "def WriteFile(context,final_summary):\n",
    "    with open('../summary.txt',mode='a', encoding='utf-8') as file:\n",
    "        file.write(\"\\n--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(f\"The original text is as follows: \\n {context}\")\n",
    "        file.write(\"\\n--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(\"--------------\\n\")\n",
    "        file.write(f\"The summarized text is as follows : \\n {final_summary}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function for Summarization\n",
    "\n",
    "1. **Purpose**: The `main` function orchestrates the text summarization process by invoking various helper functions to compute word and sentence weights, generate a summary, and write the result to a file.\n",
    "\n",
    "2. **Steps in Summarization**:\n",
    "   - **Word Weights**: It calls `get_word_weights(context)` to calculate the frequency of words, excluding stop words and punctuation, which forms the basis for sentence weighting.\n",
    "   - **Sentence Weights**: It uses `get_sentence_weights(context, word_weights)` to evaluate each sentence based on word frequencies.\n",
    "   - **Summary Generation**: It calls `get_sentence_summary(sentence_weight=sentence_weights, summary_len=0.3)` to generate a summary that covers 30% of the original content.\n",
    "\n",
    "3. **Writing Results**: After generating the summary, it calls `WriteFile(context, final_summary)` to store both the summarized and original texts in an output file.\n",
    "\n",
    "4. **Input**: The `context` parameter contains the original text that is to be summarized.\n",
    "\n",
    "5. **Output**: The function does not return any values, but it writes the summarized and original texts to a file called `summary.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(context):\n",
    "    word_weights = get_word_weights(context)\n",
    "    sentence_weights = get_sentence_weights(context,word_weights)\n",
    "    final_summary = get_sentence_summary(sentence_weight=sentence_weights, summary_len=0.5)\n",
    "    WriteFile(context,final_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample URLS for summarization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Umbriel_moon = 'https://en.wikipedia.org/wiki/Umbriel'\n",
    "Fine_tuning = \"https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)\"\n",
    "llama3  = 'https://en.wikipedia.org/wiki/Llama_(language_model)'\n",
    "hachi = f\"https://en.wikipedia.org/wiki/Hachi:_A_Dog%27s_Tale\"\n",
    "opopenheimer = \"https://en.wikipedia.org/wiki/J._Robert_Oppenheimer\"\n",
    "Beethoven = \"https://en.wikipedia.org/wiki/Ludwig_van_Beethoven\"\n",
    "context  = scraping(Fine_tuning)\n",
    "main(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Scraping and Summarization Script\n",
    "\n",
    "This script is designed to scrape Wikipedia pages, generate summaries, and save both the summary and original content in a file. The code flow can be described in the following steps:\n",
    "\n",
    "### 1. **Web Scraping with `scraping` Function**\n",
    "   - The `scraping` function sends a GET request to a Wikipedia URL.\n",
    "   - It uses the `BeautifulSoup` library to parse the HTML and extract text from all paragraph (`<p>`) elements.\n",
    "   - This extracted text (referred to as `context`) will be summarized in the subsequent steps.\n",
    "\n",
    "### 2. **Summarization Process (`main` Function)**\n",
    "   - The extracted `context` is passed to the `main(context)` function for summarization.\n",
    "   - Steps involved in summarization:\n",
    "     - **Word Frequencies**: The function `get_word_weights(context)` is used to calculate the importance of each word based on its frequency.\n",
    "     - **Sentence Weights**: Sentences are then ranked based on the importance of the words they contain using the `get_sentence_weights()` function.\n",
    "     - **Summary Creation**: A summary is generated that condenses the text to a given percentage (e.g., 30%) using `get_sentence_summary()`.\n",
    "   \n",
    "### 3. **Writing Output to File (`WriteFile` Function)**\n",
    "   - The `WriteFile` function appends the summarized text and original content to a file named `summary.txt`.\n",
    "   - The file contains:\n",
    "     - A section for the summarized text.\n",
    "     - A section for the original content, separated by multiple dashes for clarity.\n",
    "\n",
    "### 4. **Usage Example**\n",
    "   - You can scrape and summarize content from different Wikipedia pages by passing their URLs to the `scraping` function.\n",
    "   - For example, to summarize the Beethoven Wikipedia page:\n",
    "     ```python\n",
    "     context = scraping(Beethoven)\n",
    "     main(context)\n",
    "     ```\n",
    "\n",
    "### 5. **Output File**\n",
    "   - The final output is saved in `summary.txt` with the following structure:\n",
    "     ```\n",
    "     The summarized text is as follows: \n",
    "     [Summary]\n",
    "     --------------\n",
    "     --------------\n",
    "     --------------\n",
    "     --------------\n",
    "     The original text is as follows: \n",
    "     [Original Article]\n",
    "     ```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
