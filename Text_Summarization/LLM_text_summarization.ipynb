{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "549NHuoqY1Kk",
    "outputId": "cba67f44-722d-419e-9dc8-a3f1dfbadd2a"
   },
   "outputs": [],
   "source": [
    "%pip install torch numpy transformers datasets huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required libraries\n",
    "\n",
    "Hugginface key is required \n",
    "\n",
    "[Huggingface](https://huggingface.co/)\n",
    "\n",
    "use the link to signup and create a api key free of cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNH0ul7zbdi3",
    "outputId": "7e7cebf1-b11e-48f7-e67c-abaa0b9720c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\ajukh\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "huggingfacekey = \"hugginface_key\"\n",
    "login(token=huggingfacekey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Index Generation and Dataset Loading Example\n",
    "\n",
    "### Random Index Generation:\n",
    "    - Uses np.random.randint() to generate random integers.\n",
    "    - Generates n = 10 random integers between 0 and 30000.\n",
    "### CNN/DailyMail Dataset:\n",
    "    - Loaded using load_dataset(\"cnn_dailymail\", \"3.0.0\").\n",
    "    - Contains articles and summaries used for NLP tasks such as summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ty79srdoaPz_",
    "outputId": "19ccb584-6670-4566-c690-2bd8d6ed5647"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 10\n",
    "randindex = np.random.randint(30000,size=10)\n",
    "print(randindex)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting Random Articles , BART Model\n",
    "\n",
    "Random indices are generated, and the corresponding articles from the dataset are selected.\n",
    "\n",
    "\n",
    "The BART (Bidirectional and Auto-Regressive Transformer) model is loaded using Hugging Face.\n",
    "This model is used for text summarization.\n",
    "\n",
    "##### Hugging Face Pretrained Models:\n",
    "\n",
    "The \"facebook/bart-large-cnn\" model is used for summarizing news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWRwewZIdlOo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ajukh\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extracting a subset of articles from the 'train' dataset using random indices stored in 'randindex'\n",
    "articles = dataset['train'][randindex]['article']\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "# The 'facebook/bart-large-cnn' is a model specifically fine-tuned for text summarization tasks\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Loading the BART tokenizer, which is used to encode and decode text for the model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading the pre-trained BART model, which is used for generating summaries\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** This code defines a function summarize_text() that takes a text input and generates a summary using a pre-trained transformer model like BART.\n",
    "\n",
    "**Inputs:** The function accepts a text string and processes it with the tokenizer to convert it into a format suitable for the model. The input text is truncated to a maximum length of 1024 tokens to fit within the model's constraints.\n",
    "\n",
    "**Model Summarization:** It uses the modelâ€™s generate() method to create a summary with specific parameters like max_length, min_length, and beam search (num_beams=4) for optimal summary quality. Early stopping is applied to halt the generation process once a suitable summary is reached.\n",
    "\n",
    "**Output:** The function decodes the generated token IDs into a readable text summary and returns it, excluding any special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7SvjJ5ndu6Y"
   },
   "outputs": [],
   "source": [
    "# Function to summarize text using a pre-trained model\n",
    "def summarize_text(text):\n",
    "    # Tokenizing the input text, truncating to a max length of 1024 tokens, and padding if necessary\n",
    "    # The 'return_tensors' argument ensures the output is in a format compatible with PyTorch\n",
    "    inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Generating the summary using the model\n",
    "    # max_length=150 ensures the summary is not longer than 150 tokens\n",
    "    # min_length=40 ensures the summary is at least 40 tokens long\n",
    "    # length_penalty=2.0 penalizes longer summaries, encouraging conciseness\n",
    "    # num_beams=4 specifies beam search with 4 beams for better exploration of possible summary outputs\n",
    "    # early_stopping=True stops the beam search once the best summary is found\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decoding the generated summary (converting token IDs back to text)\n",
    "    # skip_special_tokens=True ensures special tokens like padding and end-of-sequence tokens are excluded\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Returning the summarized text\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)\"\n",
    "\n",
    "def scraping(url):\n",
    "    # Define browser-like headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Sending request to get the data from the webpage\n",
    "        scraped_data = requests.get(url, headers=headers)\n",
    "        scraped_data.raise_for_status()  # Check if request was successful (status code 200)\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(scraped_data.content, 'html.parser')\n",
    "\n",
    "        # Find all paragraphs and extract text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_context = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_context += p.get_text(strip=True) + \" \"  # Strip whitespace and concatenate paragraphs\n",
    "\n",
    "        return article_context.strip()  # Return the article content, ensuring no leading/trailing whitespace\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any exception raised by requests\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5cVI4WMd2uX",
    "outputId": "174fa9f2-8cca-4600-8566-0bbc3303a280"
   },
   "outputs": [],
   "source": [
    "# for i , article in enumerate(articles):\n",
    "#     summary = summarize_text(article)\n",
    "#     print(f\"Original Article {i+1}:\\n{article}\\n\")\n",
    "#     print(f\"Summary {i+1}:\\n{summary}\\n\")\n",
    "#     print(\"-\" * 80)\n",
    "#     break\n",
    "article = scraping(url)\n",
    "summary = summarize_text(article)\n",
    "def WriteFile(final_summary):\n",
    "    with open('./summary.txt',mode='a', encoding='utf-8') as file:\n",
    "        file.write(f\"\\n \\n \\n The LLM (BART) summarized text  is as follows : \\n {final_summary}\")\n",
    "WriteFile(summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
