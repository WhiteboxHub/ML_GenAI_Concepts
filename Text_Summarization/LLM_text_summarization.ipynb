{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "549NHuoqY1Kk",
        "outputId": "cba67f44-722d-419e-9dc8-a3f1dfbadd2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
            "Requirement already satisfied: numpy in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.0)\n",
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (73.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ajukh\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajukh\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ajukh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch numpy transformers datasets huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing the Required libraries\n",
        "\n",
        "Hugginface key is required \n",
        "\n",
        "[Huggingface](https://huggingface.co/)\n",
        "\n",
        "use the link to signup and create a api key free of cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNH0ul7zbdi3",
        "outputId": "7e7cebf1-b11e-48f7-e67c-abaa0b9720c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to C:\\Users\\ajukh\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from datasets import load_dataset\n",
        "huggingfacekey = \"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\"\n",
        "login(token=huggingfacekey)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Index Generation and Dataset Loading Example\n",
        "\n",
        "### Random Index Generation:\n",
        "    - Uses np.random.randint() to generate random integers.\n",
        "    - Generates n = 10 random integers between 0 and 30000.\n",
        "### CNN/DailyMail Dataset:\n",
        "    - Loaded using load_dataset(\"cnn_dailymail\", \"3.0.0\").\n",
        "    - Contains articles and summaries used for NLP tasks such as summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty79srdoaPz_",
        "outputId": "19ccb584-6670-4566-c690-2bd8d6ed5647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11553 22988  2665 17719 15816 16634 21188 27230 16303  4220]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ajukh\\.cache\\huggingface\\hub\\datasets--cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Generating train split: 100%|██████████| 287113/287113 [00:04<00:00, 65356.66 examples/s]\n",
            "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 65539.22 examples/s]\n",
            "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 66540.86 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 287113\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 13368\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'highlights', 'id'],\n",
            "        num_rows: 11490\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "n = 10\n",
        "randindex = np.random.randint(30000,size=10)\n",
        "print(randindex)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Selecting Random Articles , BART Model\n",
        "\n",
        "Random indices are generated, and the corresponding articles from the dataset are selected.\n",
        "\n",
        "\n",
        "The BART (Bidirectional and Auto-Regressive Transformer) model is loaded using Hugging Face.\n",
        "This model is used for text summarization.\n",
        "\n",
        "##### Hugging Face Pretrained Models:\n",
        "\n",
        "The \"facebook/bart-large-cnn\" model is used for summarizing news articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HWRwewZIdlOo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ajukh\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\ajukh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "articles = dataset['train'][randindex]['article']\n",
        "# Load the BART model and tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Purpose:** This code defines a function summarize_text() that takes a text input and generates a summary using a pre-trained transformer model like BART.\n",
        "\n",
        "**Inputs:** The function accepts a text string and processes it with the tokenizer to convert it into a format suitable for the model. The input text is truncated to a maximum length of 1024 tokens to fit within the model's constraints.\n",
        "\n",
        "**Model Summarization:** It uses the model’s generate() method to create a summary with specific parameters like max_length, min_length, and beam search (num_beams=4) for optimal summary quality. Early stopping is applied to halt the generation process once a suitable summary is reached.\n",
        "\n",
        "**Output:** The function decodes the generated token IDs into a readable text summary and returns it, excluding any special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p7SvjJ5ndu6Y"
      },
      "outputs": [],
      "source": [
        "# Function to summarize text\n",
        "def summarize_text(text):\n",
        "    inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True,padding=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Purpose:** This loop processes a list of articles, applies the summarize_text() function to each article, and prints both the original article and its summary.\n",
        "\n",
        "**Iteration over Articles:** The code iterates through the articles list using enumerate() to keep track of both the index (i) and the article content.\n",
        "\n",
        "**Summarization Process:** For each article, the function summarize_text(article) is called, generating a summary based on the pre-trained model (such as BART).\n",
        "\n",
        "**Output:** For each article, the code prints:\n",
        "The original article (Original Article {i+1}).\n",
        "The generated summary (Summary {i+1}).\n",
        "A separator line (\"-\" * 80) for readability between articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)\"\n",
        "\n",
        "def scraping(url):\n",
        "    # Define browser-like headers\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Sending request to get the data from the webpage\n",
        "        scraped_data = requests.get(url, headers=headers)\n",
        "        scraped_data.raise_for_status()  # Check if request was successful (status code 200)\n",
        "\n",
        "        # Parse the content with BeautifulSoup\n",
        "        soup = BeautifulSoup(scraped_data.content, 'html.parser')\n",
        "\n",
        "        # Find all paragraphs and extract text\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_context = \"\"\n",
        "\n",
        "        for p in paragraphs:\n",
        "            article_context += p.get_text(strip=True) + \" \"  # Strip whitespace and concatenate paragraphs\n",
        "\n",
        "        return article_context.strip()  # Return the article content, ensuring no leading/trailing whitespace\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Handle any exception raised by requests\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5cVI4WMd2uX",
        "outputId": "174fa9f2-8cca-4600-8566-0bbc3303a280"
      },
      "outputs": [],
      "source": [
        "# for i , article in enumerate(articles):\n",
        "#     summary = summarize_text(article)\n",
        "#     print(f\"Original Article {i+1}:\\n{article}\\n\")\n",
        "#     print(f\"Summary {i+1}:\\n{summary}\\n\")\n",
        "#     print(\"-\" * 80)\n",
        "#     break\n",
        "article = scraping(url)\n",
        "summary = summarize_text(article)\n",
        "def WriteFile(final_summary):\n",
        "    with open('./summary.txt',mode='a', encoding='utf-8') as file:\n",
        "        file.write(f\"\\n \\n \\n The LLM (BART) summarized text  is as follows : \\n {final_summary}\")\n",
        "WriteFile(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
